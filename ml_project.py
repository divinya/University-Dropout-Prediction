# -*- coding: utf-8 -*-
"""ML_Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dfqm_6btl8Vaj8DlOn7xK_Vo7a4koHs-

# 1. Data Understanding
"""

#Connect to Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
#import library
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline
#Path
#path= "/content/drive/My Drive/Colab Notebooks May"
#!ls "/content/drive/My Drive/Colab Notebooks May"

#Locate CSV data file to import
dropout_df = pd.read_csv("/content/drive/MyDrive/ML Project May 2024/student_dropout_dataset.csv")

#Displays the dataframe
#First and last 5 rows
dropout_df

#Describe data types
dropout_df.dtypes

#display the dataframe info of student dropout
#Displaying a summary of each column's data type and non-null values in the dataframe
dropout_df.info()

# Iterate through each column in the DataFrame
for column in dropout_df.select_dtypes(include=['object']).columns:
    # Print the column name and its unique values
    print(f"{column}: {dropout_df[column].unique()}")

#Dataset's statistical overview
dropout_df.describe()

#Size of the dataset
dropout_df.size

#Display shape/dimension of the dataset
#Reviewing the dataset's dimensions (total number of rows and columns)
dropout_df.shape

#Display all data of a column named
#dropout_df['ajsdsbdksabdksadbsdasd'].unique()

#Checking for missing values in the dataFrame
missing_values = dropout_df.isna().sum()
print("Missing values in each column:")
missing_values

#Identifying duplicate values in the dataset
duplicate_values = dropout_df.duplicated().sum()
print("Number of duplicate values in each column:")
duplicate_values

"""# 2. Data Preparation: Issues and Data Cleansing

# Solution 1: Renaming the columns
"""

#Renaming the column 'Nacionality' to 'Nationality' and 'Target' to 'Student Status'
#spelling correction for Nationality
dropout_df.rename(columns = {'Nacionality':'Nationality', 'Target': 'Student_Status'}, inplace = True)

dropout_df.head()

"""# Solution 2: Transform the 'Student_Status' object values into numerical format"""

#Transforming 'Student Status' values into numerical format, making them interpretable by machine learning algorithms
#Dropout = 0
#Enrolled = 1
#Graduate = 2
dropout_df['Student_Status'] = dropout_df['Student_Status'].map({'Dropout' : 0, 'Enrolled': 1, 'Graduate': 2})

#Confirmation
##Visualizing 'Student Status' numeric fields in the dataset
print(dropout_df["Student_Status"].unique())

#finds the correlation of each column in a DataFrame
dropout_df.corr()['Student_Status']

#Calculating Spearman's Rank Correlation
spearman_corr = dropout_df.corr(method='spearman')['Student_Status'].sort_values(ascending=False)

#Displaying the Spearman's Rank Correlation
print("Spearman's Rank Correlation with Student Status:")
spearman_corr

"""# Solution 3: Removing unnecessary columns"""

# Removing unnecessary columns that won't contribute to the analysis or model building
dropout_df = dropout_df.drop(columns=['Nationality', 'International', 'Educational special needs', 'Course',
                      'Mother\'s qualification','Father\'s qualification','Mother\'s occupation', 'Father\'s occupation',
                      'Curricular units 1st sem (credited)', 'Curricular units 1st sem (evaluations)',
                      'Unemployment rate', 'Inflation rate', 'GDP'], axis=1)
dropout_df.head()

"""# Solution 4: Transform the existing data into more meaningful predictors that can increase the accuracy of the model. (Feature Engineering to Enhance Predictive Power)"""

# Creating interaction features for academic performance
dropout_df['Interaction_CU_1st_2nd_Approved'] = dropout_df['Curricular units 1st sem (approved)'] * dropout_df['Curricular units 2nd sem (approved)']
dropout_df['Interaction_CU_1st_2nd_Grade'] = dropout_df['Curricular units 1st sem (grade)'] * dropout_df['Curricular units 2nd sem (grade)']

# Creating aggregated features
dropout_df['Total_CU_Approved'] = dropout_df['Curricular units 1st sem (approved)'] + dropout_df['Curricular units 2nd sem (approved)']
dropout_df['Total_CU_Grade'] = (dropout_df['Curricular units 1st sem (grade)'] + dropout_df['Curricular units 2nd sem (grade)']) / 2

# Dropping the original features to reduce multi-collinearity
columns_to_drop = ['Curricular units 1st sem (approved)', 'Curricular units 2nd sem (approved)',
                   'Curricular units 1st sem (grade)', 'Curricular units 2nd sem (grade)',]
dropout_df.drop(columns_to_drop, axis=1, inplace=True)

"""# 3. Data Preparation: Exploratory Data Analysis (EDA)

# a) Breaking Down Student Status: Graduates, Enrollments, and Dropouts Proportions

The bar chart shows that 1421 students have not continued their education, which is a large sum.
"""

# Counting the occurrences of each student status
student_status_counts = dropout_df['Student_Status'].value_counts()

# Create a bar chart
# Create a figure and axes
fig, ax = plt.subplots()

# Create a bar chart with labels
sns.countplot(x='Student_Status', data=dropout_df, ax=ax)

# Label each bar with the count, centered within the bar
for p in ax.patches:
    height = p.get_height()
    width = p.get_width()
    x = p.get_x() + width / 2
    y = height / 2
    ax.text(x, y, f"{height}", ha='center', va='center')
sns.countplot(x='Student_Status', data=dropout_df)
plt.title('Distribution of Student Status')
plt.xlabel('Student Status')
plt.ylabel('Count')
plt.show()

"""This pie chart delineates the distribution of student academic statuses within the dataset. It reveals that 32.12% of students have not continued their education, highlighting a dropout rate that warrants attention and intervention."""

# Mapping student status labels (optional, use if needed)
student_status_labels = {0: 'Dropouts', 1: 'Enrolled', 2: 'Graduates'}

# Get student status counts
categories_counts = dropout_df['Student_Status'].value_counts()

# Use descriptive labels if defined
if student_status_labels:
    labels = [student_status_labels[i] for i in categories_counts.index]
else:
    labels = categories_counts.index  # Use numerical labels if no mapping provided

# Create the pie chart
plt.pie(categories_counts.values, labels=labels, autopct='%1.2f%%')
plt.title('Educational Outcomes: Distribution of Graduates (2), Enrolled Students (1), and Dropouts (0).')
plt.show()

"""# b) Comprehensive Analysis of Factors Influencing Student Status within an academic setting

- A correlation matrix heatmap elucidates the intricate relationships between different variables.
"""

# Analyzing the correlation between various features with a heatmap to identify potential predictors for Student Status

plt.figure(figsize=(30,30))
ax = sns.heatmap(dropout_df.corr(), annot=True, cmap='coolwarm')
plt.show()

"""- A bar graph of the top 10 features with the highest correlation to Student Status allows for an immediate identification of the most significant predictors."""

# Recalculating correlations to ensure consistency
correlations = dropout_df.corr()['Student_Status'].drop('Student_Status')
top_10_features = correlations.abs().nlargest(10).index
top_10_corr_values = correlations[top_10_features].values

# Plotting the top 10 features
plt.figure(figsize=(10, 8))
plt.bar(x=top_10_features, height=top_10_corr_values, color='skyblue')
plt.xlabel('Features')
plt.ylabel('Correlation with Student Status')
plt.title('Top 10 Features with Highest Correlation to Student Status')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

"""
- A Factors Ranker heatmap ranks these features and also provides an aesthetic and ordered visualization."""

# Creating a new DataFrame with only the top 10 features and 'Student Status'
top_10_df = dropout_df[top_10_features.to_list() + ['Student_Status']].copy()

# Computing the correlation matrix for these features
top_10_corr = top_10_df.corr()[['Student_Status']]

# Sorting by absolute value while keeping the sign to place negative values below positives
sorted_corr = top_10_corr.sort_values(by='Student_Status', key=lambda x: abs(x), ascending=False)

# Plotting the heatmap with custom sorting
plt.figure(figsize=(5, 10))
sns.heatmap(sorted_corr, annot=True, center=0)
plt.title('Factors Ranker')
plt.show()

"""# c) Academic Performance and Its Impact on Student Status

- Impact of Interaction Between 1st and 2nd Semester Grades on Student Status
"""

# Box plot of Interaction of 1st and 2nd Semester Grades by Student Status
plt.figure(figsize=(10, 6))
sns.boxplot(x='Student_Status', y='Interaction_CU_1st_2nd_Grade', data=dropout_df, palette='pastel')
plt.xticks([0, 1, 2], ['Dropouts', 'Enrolled', 'Graduates'])
plt.title('Box Plot of Impact of Grade Interaction on Student Status')
plt.xlabel('Student Status')
plt.ylabel('Interaction of 1st and 2nd Semester Grades')
plt.show()

plt.figure(figsize=(10, 6))
sns.barplot(x='Student_Status', y='Interaction_CU_1st_2nd_Grade', data=dropout_df, errorbar=None)
plt.xticks([0, 1, 2], ['Dropouts', 'Enrolled', 'Graduates'])
plt.xlabel('Student Status')
plt.ylabel('Interaction of 1st and 2nd Semester Grades')
plt.title('Impact of Grade Interaction on Student Status')
# Loop through bars and add count labels within the bars
for bar in plt.gca().patches:
    height = bar.get_height()
    x = bar.get_x() + bar.get_width() / 2
    y = bar.get_height() / 2  # Calculate the y-coordinate for the center of the bar
    plt.text(x, y, f"{int(height)}", ha='center', va='center')  # Center the text within the bar

plt.show()
plt.show()

"""- Impact of Total Approved Curricular Units on Student Status"""

# Box plot of Total_CU_Approved by Student Status
plt.figure(figsize=(10, 6))
sns.boxplot(x='Student_Status', y='Total_CU_Approved', data=dropout_df, palette='pastel')
plt.xticks([0, 1, 2], ['Dropouts', 'Enrolled', 'Graduates'])
plt.title('Box Plot of Total Approved Curricular on Student Status')
plt.xlabel('Student Status')
plt.ylabel('Total_CU_Approved')
plt.show()

plt.figure(figsize=(10, 6))
sns.barplot(x='Student_Status', y='Total_CU_Approved', data=dropout_df, errorbar=None)
plt.xticks([0, 1, 2], ['Dropouts', 'Enrolled', 'Graduates'])
plt.xlabel('Student Status')
plt.ylabel('Total Curricular Units Approved')
plt.title('Impact of Total Approved Curricular Units on Student Status')
# Loop through bars and add count labels within the bars
for bar in plt.gca().patches:
    height = bar.get_height()
    x = bar.get_x() + bar.get_width() / 2
    y = bar.get_height() / 2  # Calculate the y-coordinate for the center of the bar
    plt.text(x, y, f"{int(height)}", ha='center', va='center')  # Center the text within the bar

plt.show()

"""# d) Assessing the Role of Financial Status in Educational Progress

- Debt and Educational Trajectories
"""

import seaborn as sns
import matplotlib.pyplot as plt

# Mapping Student Status numerical labels to descriptive labels for the legend
student_status_labels = {0: 'Dropouts', 1: 'Enrolled', 2: 'Graduated'}

# Mapping numerical labels to descriptive labels for Debtor status
debtor_status_labels = {0: 'No Debt', 1: 'In Debt'}

# Visualizing the impact of student debt on educational status
grouped = dropout_df.groupby(['Debtor', 'Student_Status']).size().unstack()
grouped.rename(index=debtor_status_labels, inplace=True)

# Create the stacked bar chart
grouped_plot = grouped.plot(kind='bar', stacked=True)

# Add count labels within the bars
for container in grouped_plot.containers:
    for patch in container.patches:
        height = patch.get_height()
        width = patch.get_width()
        x = patch.get_x() + width / 2
        y = patch.get_y() + height / 2
        plt.text(x, y, f'{int(height)}', ha='center', va='center', color='white', fontsize=10)

plt.xlabel('Debtor Status')
plt.ylabel('Count')
plt.title('Impact of Financial Obligations on Student Educational Status')
plt.legend(labels=[student_status_labels.get(item, item) for item in grouped.columns])
plt.xticks(rotation=0)
plt.show()

# Mapping Student Status numerical labels to descriptive labels for the legend
student_status_labels = {0: 'Dropouts', 1: 'Enrolled', 2: 'Graduated'}

# Mapping numerical labels to descriptive labels for Debtor status
debtor_status_labels = {0: 'No Debt', 1: 'In Debt'}

# Visualizing the impact of student debt on educational status
grouped = dropout_df.groupby(['Student_Status', 'Debtor']).size().unstack()
grouped.rename(index=student_status_labels, inplace=True)

# Create the stacked bar chart
grouped_plot = grouped.plot(kind='bar', stacked=True)

# Add count labels within the bars
for container in grouped_plot.containers:
    for patch in container.patches:
        # Calculate bar height and width
        height = patch.get_height()
        width = patch.get_width()

        # Calculate center coordinates (x, y) for text placement
        x = patch.get_x() + width / 2
        y = patch.get_y() + height / 2

        # Add text with count, color, and font size
        plt.text(x, y, f'{int(height)}', ha='center', va='center', color='white', fontsize=10)

plt.xlabel('Student Status')
plt.ylabel('Count')
plt.title('Impact of Financial Obligations on Student Educational Status')
plt.legend(labels=[debtor_status_labels.get(item, item) for item in grouped.columns])
plt.xticks(rotation=0)
plt.show()

"""- Scholarships as a Support System"""

# Mapping numerical labels to descriptive labels for Scholarship status
scholarship_status_labels = {0: 'No Scholarship', 1: 'Scholarship Holder'}

# Visualizing the impact of scholarships on educational status
grouped = dropout_df.groupby(['Scholarship holder', 'Student_Status']).size().unstack()
grouped.rename(index=scholarship_status_labels, inplace=True)
grouped_plot = grouped.plot(kind='bar', stacked=True)

# Add count labels within the bars
for container in grouped_plot.containers:
    for patch in container.patches:
        height = patch.get_height()
        width = patch.get_width()
        x = patch.get_x() + width / 2
        y = patch.get_y() + height / 2
        plt.text(x, y, f'{int(height)}', ha='center', va='center', color='white', fontsize=10)

plt.xlabel('Scholarship Status')
plt.ylabel('Count')
plt.title('Impact of Financial Aids on Student Educational Status')
plt.legend(labels=[student_status_labels.get(item, item) for item in grouped.columns])
plt.xticks(rotation=0)
plt.show()

# Mapping numerical labels to descriptive labels for Scholarship status
scholarship_status_labels = {0: 'No Scholarship', 1: 'Scholarship Holder'}

# Visualizing the impact of scholarships on educational status
grouped = dropout_df.groupby(['Student_Status', 'Scholarship holder']).size().unstack()
grouped.rename(index=student_status_labels, inplace=True)
grouped_plot = grouped.plot(kind='bar', stacked=True)

# Add count labels within the bars
for container in grouped_plot.containers:
    for patch in container.patches:
        # Calculate bar height and width
        height = patch.get_height()
        width = patch.get_width()

        # Calculate center coordinates (x, y) for text placement
        x = patch.get_x() + width / 2
        y = patch.get_y() + height / 2

        # Add text with count, color, and font size
        plt.text(x, y, f'{int(height)}', ha='center', va='center', color='white', fontsize=10)

plt.xlabel('Student Status')
plt.ylabel('Count')
plt.title('Impact of Financial Aids on Student Educational Status')
plt.legend(labels=[scholarship_status_labels.get(item, item) for item in grouped.columns])
plt.xticks(rotation=0)
plt.show()

"""- Tuition Fees: A Predictive Financial Indicator"""

# Mapping numerical labels to descriptive labels for Tuition Fee status
tuition_fee_status_labels = {0: 'Fees Not Up to Date', 1: 'Fees Up to Date'}

# Visualizing the impact of tuition fees status on educational outcomes
grouped = dropout_df.groupby(['Tuition fees up to date', 'Student_Status']).size().unstack()
grouped.rename(index=tuition_fee_status_labels, inplace=True)
grouped_plot = grouped.plot(kind='bar', stacked=True)

# Add count labels within the bars
for container in grouped_plot.containers:
    for patch in container.patches:
        height = patch.get_height()
        width = patch.get_width()
        x = patch.get_x() + width / 2
        y = patch.get_y() + height / 2
        plt.text(x, y, f'{int(height)}', ha='center', va='center', color='white', fontsize=10)

plt.xlabel('Tuition Fee Status')
plt.ylabel('Count')
plt.title('Impact of Tuition Fees Status on Student Educational Status')
plt.legend(labels=[student_status_labels.get(item, item) for item in grouped.columns])
plt.xticks(rotation=0)
plt.show()

# Mapping numerical labels to descriptive labels for Tuition Fee status
tuition_fee_status_labels = {0: 'Fees Not Up to Date', 1: 'Fees Up to Date'}

# Visualizing the impact of tuition fees status on educational outcomes
grouped = dropout_df.groupby(['Student_Status', 'Tuition fees up to date']).size().unstack()
grouped.rename(index=student_status_labels, inplace=True)
grouped_plot = grouped.plot(kind='bar', stacked=True)

# Add count labels within the bars
for container in grouped_plot.containers:
    for patch in container.patches:
        # Calculate bar height and width
        height = patch.get_height()
        width = patch.get_width()

        # Calculate center coordinates (x, y) for text placement
        x = patch.get_x() + width / 2
        y = patch.get_y() + height / 2

        # Add text with count, color, and font size
        plt.text(x, y, f'{int(height)}', ha='center', va='center', color='white', fontsize=10)

plt.xlabel('Student Status')
plt.ylabel('Count')
plt.title('Impact of Tuition Fees Status on Student Educational Status')
plt.legend(labels=[tuition_fee_status_labels.get(item, item) for item in grouped.columns])
plt.xticks(rotation=0)
plt.show()

"""# e) The Interplay of Age and Gender in Academic Progression

- Age Distribution Analysis
"""

# Create a histogram
plt.figure(figsize=(15, 7))
sns.histplot(dropout_df['Age at enrollment'], bins=50, color='b', kde=True)
plt.title('Age at Enrollment Distribution')
plt.xlabel("Age at Enrollment")
plt.ylabel('Count')
plt.show()

# Converting inf to NaN in the 'Age at enrollment' column
dropout_df['Age at enrollment'] = dropout_df['Age at enrollment'].replace([np.inf, -np.inf], np.nan)

# Ignoring FutureWarnings
import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)

plt.figure(figsize=(15, 7))
ax = sns.histplot(dropout_df['Age at enrollment'], bins=50, color='g', kde=True)
ax.set_title('Age at Enrollment Distribution')
ax.set_xlabel('Age at Enrollment')
ax.set_ylabel('Count')
plt.show()

"""- Dynamics of Age in Student Outcomes"""

plt.figure(figsize=(10, 6))
sns.barplot(x='Student_Status', y='Age at enrollment', data=dropout_df, errorbar=None)
plt.xticks([0, 1, 2], ['Dropouts', 'Enrolled', 'Graduates'])
plt.xlabel('Student Status')
plt.ylabel('Age at enrollment')
plt.title('Relationship Between Age at Enrollment and Student Status')
# Loop through bars and add count labels within the bars
for bar in plt.gca().patches:
    height = bar.get_height()
    x = bar.get_x() + bar.get_width() / 2
    y = bar.get_height() / 2  # Calculate the y-coordinate for the center of the bar
    plt.text(x, y, f"{int(height)}", ha='center', va='center')  # Center the text within the bar

plt.show()

plt.figure(figsize=(10, 8))
sns.boxplot(x='Student_Status', y='Age at enrollment', data=dropout_df)
plt.xticks([0, 1, 2], ['Dropouts', 'Enrolled', 'Graduates'])
plt.xlabel('Student Status')
plt.ylabel('Age at Enrollment')
plt.title('Relationship Between Age at Enrollment and Student Status')
plt.show()

"""- Gender's Influence on Educational Trajectories"""

plt.figure(figsize=(17, 6))
sns.countplot(x='Student_Status', hue='Gender', data=dropout_df, palette= sns.color_palette('Set1'))
plt.xlabel('Student Status (0: Dropouts, 1: Enrolled, 2: Graduates)')
plt.ylabel('Count')
plt.legend(title='Gender', labels=['0: Female', ' 1: Male'])
plt.title('Influence of Gender on Student Status')
# Add count labels within the bars
for container in plt.gca().containers:
    for patch in container.patches:
        height = patch.get_height()
        width = patch.get_width()
        x = patch.get_x() + width / 2
        y = patch.get_y() + height / 2
        plt.text(x, y, f'{int(height)}', ha='center', va='center', color='white', fontsize=10)
plt.show()

plt.figure(figsize=(17, 6))
sns.countplot(x='Gender', hue='Student_Status', data=dropout_df, palette= sns.color_palette('Set2'))
plt.xlabel('Gender (0: Female, 1: Male)')
plt.ylabel('Count')
plt.legend(title='Student Status', labels=['Dropouts', 'Enrolled', 'Graduates'])
plt.title('Influence of Gender on Student Status')
# Add count labels within the bars
for container in plt.gca().containers:
    for patch in container.patches:
        height = patch.get_height()
        width = patch.get_width()
        x = patch.get_x() + width / 2
        y = patch.get_y() + height / 2
        plt.text(x, y, f'{int(height)}', ha='center', va='center', color='white', fontsize=10)
plt.show()

"""# 4. Modelling"""

from sklearn.model_selection import train_test_split

# Preparing the data: Separating features and target label
X = dropout_df.drop('Student_Status', axis=1)
y = dropout_df['Student_Status']

# Splitting the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import numpy as np
from sklearn.preprocessing import StandardScaler

# Standardizing features by removing the mean and scaling to unit variance
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)

# Fitting PCA on the training
pca = PCA().fit(X_train_scaled)

# Visualizing the cumulative explained variance to determine the number of components needed
plt.figure()
plt.plot(np.arange(1, len(pca.explained_variance_ratio_) + 1),
         np.cumsum(pca.explained_variance_ratio_),
         marker='o', linestyle='--')
plt.title('Explained Variance by Different Principal Components')
plt.xlabel('Number of Components')
plt.ylabel('Cumulative Explained Variance')
plt.grid(True)

# Drawing a horizontal line at 95% cumulative explained variance
plt.axhline(y=0.95, color='r', linestyle='-')
plt.text(0.5, 0.91, '95% cut-off threshold', color = 'red', fontsize=12)

plt.xticks(np.arange(1, len(pca.explained_variance_ratio_) + 1, step=1))

plt.grid(True)
plt.tight_layout()
plt.show()

from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import SMOTE
from imblearn.pipeline import Pipeline as ImbPipeline
from sklearn.decomposition import PCA
from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from scipy.stats import randint, uniform
from sklearn.model_selection import learning_curve

# Configuring different machine learning models with hyperparameters and pipelines
models_config = {
    'Gradient Boosting': {
        'pipeline': ImbPipeline(steps=[
            ('scaler', StandardScaler()),
            ('smote', SMOTE(random_state=42)),
            ('pca', PCA(n_components=0.95, random_state=42)),
            ('model', GradientBoostingClassifier(random_state=42))
        ]),
        'params': {
            'model__n_estimators': [100, 150, 200],
            'model__learning_rate': [0.01, 0.05],
            'model__max_depth': [2, 3, 4],
            'model__subsample': [0.5, 0.75, 1.0],
            'model__validation_fraction': [0.1, 0.2],
            'model__n_iter_no_change': [10, 20, 30],
            'model__tol': [1e-4],
        }
    },
    'Random Forest': {
        'pipeline': ImbPipeline(steps=[
            ('scaler', StandardScaler()),
            ('smote', SMOTE(random_state=42)),
            ('pca', PCA(n_components=0.95, random_state=42)),
            ('model', RandomForestClassifier(random_state=42))
        ]),
        'params': {
            'model__n_estimators': [100, 125],
            'model__max_depth': [3, 4],
            'model__min_samples_split': [4, 6],
            'model__min_samples_leaf': [2, 3],
            'model__max_features': ['log2'],
        }
    },
    'Logistic Regression': {
        'pipeline': ImbPipeline(steps=[
            ('scaler', StandardScaler()),
            ('smote', SMOTE(random_state=42)),
            ('pca', PCA(n_components=0.95, random_state=42)),
            ('model', LogisticRegression(random_state=42))
        ]),
        'params': {
            'model__C': [0.005, 0.01, 0.05],
            'model__penalty': ['l1', 'l2'],
            'model__solver': ['liblinear'],
        }
    },
    'SVM': {
        'pipeline': ImbPipeline(steps=[
            ('scaler', StandardScaler()),
            ('smote', SMOTE(random_state=42)),
            ('pca', PCA(n_components=0.95, random_state=42)),
            ('model', SVC(probability=True, random_state=42))
        ]),
        'params': {
            'model__C': uniform(0.5, 2),
            'model__kernel': ['rbf', 'linear'],
        }
    },
    'KNN': {
        'pipeline': ImbPipeline(steps=[
            ('scaler', StandardScaler()),
            ('smote', SMOTE(random_state=42)),
            ('pca', PCA(n_components=0.95, random_state=42)),
            ('model', KNeighborsClassifier())
        ]),
        'params': {
            'model__n_neighbors': randint(8, 14),
            'model__weights': ['uniform'],
        }
    }
}

# Configuring learning curves to identify if the models are underfitting or overfitting
def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,
                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):
    plt.figure()
    plt.title(title)
    if ylim is not None:
        plt.ylim(*ylim)
    plt.xlabel("Training examples")
    plt.ylabel("Score")
    train_sizes, train_scores, test_scores = learning_curve(
        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)
    train_scores_mean = np.mean(train_scores, axis=1)
    train_scores_std = np.std(train_scores, axis=1)
    test_scores_mean = np.mean(test_scores, axis=1)
    test_scores_std = np.std(test_scores, axis=1)

    plt.grid()

    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,
                     train_scores_mean + train_scores_std, alpha=0.1,
                     color="r")
    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,
                     test_scores_mean + test_scores_std, alpha=0.1, color="g")
    plt.plot(train_sizes, train_scores_mean, 'o-', color="r",
             label="Training score")
    plt.plot(train_sizes, test_scores_mean, 'o-', color="g",
             label="Cross-validation score")

    plt.legend(loc="best")
    plt.show()

from sklearn.model_selection import RandomizedSearchCV
import time
from datetime import timedelta

best_models = {}
start_time = time.time()

# Performing RandomizedSearchCV
for name, config in models_config.items():
    print(f"Starting search for: {name}")
    search_start_time = time.time()

    search = RandomizedSearchCV(config['pipeline'], config['params'], n_iter=6, cv=5, scoring='accuracy', random_state=42, verbose=1)
    search.fit(X_train, y_train)

    best_models[name] = search.best_estimator_
    print(f"Test score for {name}: {search.best_score_:.4f}")

    # Comparing training set performance to the best CV score
    training_score = search.score(X_train, y_train)
    print(f"Training Score for {name}: {training_score:.4f}")

    elapsed_time = int(time.time() - search_start_time)
    print(f"Search completed for {name}. Time taken: {str(timedelta(seconds=elapsed_time))}")

    # Displaying the best parameters for the current model
    print(f"Best parameters for {name}: {search.best_params_}")

    # Plotting the learning curve for the best estimator
    plot_learning_curve(search.best_estimator_, f"Learning Curves ({name})", X_train, y_train, ylim=(0.5, 1.01), cv=5, n_jobs=4)

overall_time = int(time.time() - start_time)
print(f"Total time for all model searches: {str(timedelta(seconds=overall_time))}")

from sklearn.ensemble import VotingClassifier

# Combining multiple models into an ensemble VotingClassifier to improve prediction accuracy
ensemble = VotingClassifier(estimators=[(name, model) for name, model in best_models.items()],
                            voting='soft') # 'soft' voting considers the confidence level of predictions

ensemble.fit(X_train, y_train)

from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

# Evaluating ensemble model performance on the test set
y_pred = ensemble.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
print(f"Ensemble model accuracy: {accuracy:.4f}")

# Plotting the Confusion Matrix
conf_matrix = confusion_matrix(y_test, y_pred)
sns.heatmap(conf_matrix, annot=True, fmt="d")
plt.title('Confusion Matrix')
plt.ylabel('Actual label')
plt.xlabel('Predicted label')
plt.show()

# Overfitting Check
train_accuracy = ensemble.score(X_train, y_train)
test_accuracy = ensemble.score(X_test, y_test)

print(f"Test Accuracy: {test_accuracy:.4f}")
print(f"Training Accuracy: {train_accuracy:.4f}\n")

# Classification Report
print(classification_report(y_test, y_pred, target_names=['Dropout', 'Enrolled', 'Graduate']))

from sklearn.metrics import roc_curve, auc
from sklearn.preprocessing import label_binarize
from itertools import cycle

class_names = ['Dropout', 'Enrolled', 'Graduate']

# Binarizing the output labels for multi-class plotting
y_test_binarized = label_binarize(y_test, classes=np.unique(y))
n_classes = y_test_binarized.shape[1]

# Getting the probabilities for each class from your trained ensemble model
y_prob = ensemble.predict_proba(X_test)

# Computing ROC curve and ROC area for each class
fpr = dict()
tpr = dict()
roc_auc = dict()
for i in range(n_classes):
    fpr[i], tpr[i], _ = roc_curve(y_test_binarized[:, i], y_prob[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

# Plotting ROC curves.
plt.figure(figsize=(10, 8))
colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])
for i, color in zip(range(n_classes), colors):
    plt.plot(fpr[i], tpr[i], color=color, lw=2,
             label=f'ROC curve of class {class_names[i]} (area = {roc_auc[i]:0.2f})')

plt.plot([0, 1], [0, 1], 'k--', lw=2)
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic for Multi-Class')
plt.legend(loc="lower right")
plt.show()

from sklearn.metrics import precision_recall_curve

# Calculating precision and recall for each class
precision = dict()
recall = dict()
average_precision = dict()
for i in range(n_classes):
    precision[i], recall[i], _ = precision_recall_curve(y_test_binarized[:, i], y_prob[:, i])
    average_precision[i] = auc(recall[i], precision[i])

# Plotting the precision-recall curves
plt.figure(figsize=(7, 6))
colors = cycle(['navy', 'turquoise', 'darkorange', 'cornflowerblue', 'teal'])

for i, color in zip(range(n_classes), colors):
    plt.plot(recall[i], precision[i], color=color, lw=2,
             label=f'{class_names[i]}'
             ''.format(i, average_precision[i]))

plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall curves')
plt.legend(loc="upper right")
plt.show()

from sklearn.manifold import TSNE
from sklearn.utils import resample

# Subsampling the data
X_sampled, y_sampled = resample(X_train, y_train, n_samples=1000, stratify=y_train, random_state=42)

# Running t-SNE
tsne = TSNE(n_components=2, perplexity=30, n_iter=1000, random_state=42)
X_reduced = tsne.fit_transform(X_sampled)

# Predicting with the ensemble model
y_pred = ensemble.predict(X_sampled)

# Replacing numeric labels with string labels
label_names = {0: 'Dropout', 1: 'Enrolled', 2: 'Graduate'}
y_sampled_named = y_sampled.map(label_names)

# Adjusting the color palette to have clearer distinctions
palette = sns.color_palette("bright", len(label_names))

# Modifying the legend mapping for numeric to class names
legend_labels = {v: k for k, v in label_names.items()}

# 2D t-SNE Plot
sns.set(style="whitegrid", palette="muted")
plt.figure(figsize=(10, 8))
sns.scatterplot(x=X_reduced[:, 0], y=X_reduced[:, 1], hue=y_sampled.map(label_names),
                palette=palette, alpha=0.6, s=50)

plt.title('2D t-SNE visualization of Student Status')
plt.xlabel('t-SNE feature 1')
plt.ylabel('t-SNE feature 2')
plt.legend(title='Student Status')
plt.show()

# Cloning the original DataFrame to preserve the initial dataset
df_rethinked = dropout_df.copy()

# Transforming 'Student Status' into a binary 'dropout' feature
df_rethinked['dropout'] = df_rethinked['Student_Status'].map(lambda x: 1 if x == 0 else 0)

# Dropping the original 'Student Status' to avoid confusion
df_rethinked.drop(['Student_Status'], axis=1, inplace=True)

# Preparing the dataset for training
X_new = df_rethinked.drop('dropout', axis=1)
y_new = df_rethinked['dropout']

# Splitting the dataset into training and testing sets for the binary classification
X_train_new, X_test_new, y_train_new, y_test_new = train_test_split(X_new, y_new, test_size=0.2, random_state=42)

# Model configurations with hyperparameters
models_config_new = {
    'Gradient Boosting': {
        'pipeline': ImbPipeline(steps=[
            ('scaler', StandardScaler()),
            ('smote', SMOTE(random_state=42)),
            ('pca', PCA(n_components=0.95)),
            ('model', GradientBoostingClassifier(random_state=42))
        ]),
        'params': {
            'model__n_estimators': randint(100, 150),
            'model__learning_rate': uniform(0.01, 0.1),
            'model__max_depth': randint(2, 4)
        }
    },
    'Random Forest': {
        'pipeline': ImbPipeline(steps=[
            ('scaler', StandardScaler()),
            ('smote', SMOTE(random_state=42)),
            ('pca', PCA(n_components=0.95)),
            ('model', RandomForestClassifier(random_state=42))
        ]),
        'params': {
            'model__n_estimators': randint(100, 200),
            'model__max_depth': randint(3, 6),
            'model__min_samples_split': randint(3, 6),
            'model__min_samples_leaf': randint(2, 4)
        }
    },
    'Logistic Regression': {
        'pipeline': ImbPipeline(steps=[
            ('scaler', StandardScaler()),
            ('smote', SMOTE(random_state=42)),
            ('pca', PCA(n_components=0.95)),
            ('model', LogisticRegression(random_state=42))
        ]),
        'params': {
            'model__C': uniform(0.05, 1)
        }
    },
    'SVM': {
        'pipeline': ImbPipeline(steps=[
            ('scaler', StandardScaler()),
            ('smote', SMOTE(random_state=42)),
            ('pca', PCA(n_components=0.95)),
            ('model', SVC(probability=True, random_state=42))
        ]),
        'params': {
            'model__C': uniform(1, 2),
            'model__kernel': ['rbf']
        }
    },
    'KNN': {
        'pipeline': ImbPipeline(steps=[
            ('scaler', StandardScaler()),
            ('smote', SMOTE(random_state=42)),
            ('pca', PCA(n_components=0.95)),
            ('model', KNeighborsClassifier())
        ]),
        'params': {
            'model__n_neighbors': randint(20, 30),
            'model__weights': ['uniform', 'distance']
        }
    }
}

best_models_binary = {}
start_time_new = time.time()

# New RandomizedSearchCV
for name, config in models_config_new.items():
    print(f"Starting search for: {name}")
    search_start_time_new = time.time()

    search = RandomizedSearchCV(config['pipeline'], config['params'], n_iter=12, cv=5, scoring='accuracy', random_state=42, verbose=1)
    search.fit(X_train_new, y_train_new)

    best_models_binary[name] = search.best_estimator_
    print(f"Test score for {name}: {search.best_score_:.4f}")

    # Comparing training set performance to the best CV score
    training_score = search.score(X_train_new, y_train_new)
    print(f"Training Score for {name}: {training_score:.4f}")

    elapsed_time_new = int(time.time() - search_start_time_new)
    print(f"Search completed for {name}. Time taken: {str(timedelta(seconds=elapsed_time_new))}")

    # Displaying the best parameters for the current model
    print(f"Best parameters for {name}: {search.best_params_}")

    # Plotting the learning curve for the best estimator
    plot_learning_curve(search.best_estimator_, f"Learning Curves ({name})", X_train_new, y_train_new, ylim=(0.5, 1.01), cv=5, n_jobs=4)

overall_time_new = int(time.time() - start_time_new)
print(f"Total time for all model searches: {str(timedelta(seconds=overall_time_new))}")

import matplotlib.pyplot as plt
import numpy as np

# Dictionary to store the test accuracy scores for all models
test_scores = {}

# Model configurations with hyperparameters
models_config_new = {
    'Gradient Boosting': {
        'pipeline': ImbPipeline(steps=[
            ('scaler', StandardScaler()),
            ('smote', SMOTE(random_state=42)),
            ('pca', PCA(n_components=0.95)),
            ('model', GradientBoostingClassifier(random_state=42))
        ]),
        'params': {
            'model__n_estimators': randint(100, 150),
            'model__learning_rate': uniform(0.01, 0.1),
            'model__max_depth': randint(2, 4)
        }
    },
    'Random Forest': {
        'pipeline': ImbPipeline(steps=[
            ('scaler', StandardScaler()),
            ('smote', SMOTE(random_state=42)),
            ('pca', PCA(n_components=0.95)),
            ('model', RandomForestClassifier(random_state=42))
        ]),
        'params': {
            'model__n_estimators': randint(100, 200),
            'model__max_depth': randint(3, 6),
            'model__min_samples_split': randint(3, 6),
            'model__min_samples_leaf': randint(2, 4)
        }
    },
    'Logistic Regression': {
        'pipeline': ImbPipeline(steps=[
            ('scaler', StandardScaler()),
            ('smote', SMOTE(random_state=42)),
            ('pca', PCA(n_components=0.95)),
            ('model', LogisticRegression(random_state=42))
        ]),
        'params': {
            'model__C': uniform(0.05, 1)
        }
    },
    'SVM': {
        'pipeline': ImbPipeline(steps=[
            ('scaler', StandardScaler()),
            ('smote', SMOTE(random_state=42)),
            ('pca', PCA(n_components=0.95)),
            ('model', SVC(probability=True, random_state=42))
        ]),
        'params': {
            'model__C': uniform(1, 2),
            'model__kernel': ['rbf']
        }
    },
    'KNN': {
        'pipeline': ImbPipeline(steps=[
            ('scaler', StandardScaler()),
            ('smote', SMOTE(random_state=42)),
            ('pca', PCA(n_components=0.95)),
            ('model', KNeighborsClassifier())
        ]),
        'params': {
            'model__n_neighbors': randint(20, 30),
            'model__weights': ['uniform', 'distance']
        }
    }
}

best_models_binary = {}
start_time_new = time.time()

# New RandomizedSearchCV
for name, config in models_config_new.items():
    print(f"Starting search for: {name}")
    search_start_time_new = time.time()

    search = RandomizedSearchCV(
        config['pipeline'],
        config['params'],
        n_iter=12,
        cv=5,
        scoring='accuracy',
        random_state=42,
        verbose=1
    )
    search.fit(X_train_new, y_train_new)

    best_models_binary[name] = search.best_estimator_
    print(f"Test score for {name}: {search.best_score_:.4f}")

    # Storing test accuracy score
    test_scores[name] = search.best_score_

    elapsed_time_new = int(time.time() - search_start_time_new)
    print(f"Search completed for {name}. Time taken: {str(timedelta(seconds=elapsed_time_new))}")

    # Displaying the best parameters for the current model
    print(f"Best parameters for {name}: {search.best_params_}")

overall_time_new = int(time.time() - start_time_new)
print(f"Total time for all model searches: {str(timedelta(seconds=overall_time_new))}")

# Plotting the accuracy scores
fig, ax = plt.subplots(figsize=(10, 6))
models = list(test_scores.keys())
scores = list(test_scores.values())

# Bar chart
bars = plt.bar(models, scores, color='skyblue')

# Adding the accuracy scores on top of the bars
for bar in bars:
    yval = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2.0, yval, round(yval, 4), va='bottom') # va: vertical alignment

plt.xlabel('Models')
plt.ylabel('Accuracy Score')
plt.title('Model Comparison: Accuracy Scores')
plt.ylim(0, 1)  # Assuming accuracy scores are between 0 and 1
plt.show()

from sklearn.ensemble import VotingClassifier

# Creating an ensemble of the best models
ensemble_binary = VotingClassifier(
    estimators=[(name, model) for name, model in best_models_binary.items()],
    voting='soft'
)
ensemble_binary.fit(X_train_new, y_train_new)

# Predictions
y_pred_binary = ensemble_binary.predict(X_test_new)

# Accuracy
accuracy_binary = accuracy_score(y_test_new, y_pred_binary)
print(f"Ensemble Model Accuracy: {accuracy_binary:.4f}")

# Confusion Matrix
conf_matrix_ensemble_binary = confusion_matrix(y_test_new, y_pred_binary)
sns.heatmap(conf_matrix_ensemble_binary, annot=True, fmt="d", cmap='Blues')
plt.title(f"Confusion Matrix - Ensemble Model\nAccuracy: {accuracy_binary:.4f}")
plt.xlabel("Predicted label")
plt.ylabel("True label")
plt.show()

# Classification Report
print(classification_report(y_test_new, y_pred_binary, target_names=['Dropout', 'No Dropout']))

# ROC AUC Curve
y_prob_binary = ensemble_binary.predict_proba(X_test_new)[:, 1]
fpr, tpr, thresholds = roc_curve(y_test_new, y_prob_binary)
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(10, 8))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic')
plt.legend(loc="lower right")
plt.show()

from sklearn.metrics import average_precision_score

# Precision-Recall Curve for binary classification
precision, recall, _ = precision_recall_curve(y_test_new, y_prob_binary)
average_precision = average_precision_score(y_test_new, y_prob_binary)

plt.figure(figsize=(7, 6))
plt.step(recall, precision, color='b', alpha=0.2, where='post')
plt.fill_between(recall, precision, step='post', alpha=0.2, color='b')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.ylim([0.0, 1.05])
plt.xlim([0.0, 1.0])
plt.title(f'2-class Precision-Recall curve: AP={average_precision:.2f}')
plt.show()

# 2D t-SNE Visualization for binary classification
X_reduced = TSNE(n_components=2, random_state=42).fit_transform(X_test_new)

# Replacing numeric labels with string labels
label_names = {0: 'No Dropout', 1: 'Dropout'}
y_test_named = y_test_new.map(label_names)

# Adjusting the color palette to have clear distinctions
palette = sns.color_palette("bright", len(label_names))

# Modifying the legend mapping for numeric to class names
legend_labels = {v: k for k, v in label_names.items()}

# 2D t-SNE Plot
sns.set(style="whitegrid", palette="muted")
plt.figure(figsize=(10, 8))
sns.scatterplot(x=X_reduced[:, 0], y=X_reduced[:, 1], hue=y_test_named, palette=palette, alpha=0.6, s=50)
plt.title('2D t-SNE visualization of Student Status')
plt.xlabel('t-SNE feature 1')
plt.ylabel('t-SNE feature 2')
plt.legend(title='Student Status', loc='best')
plt.show()

# Visualization setup
labels = ['Original Model', 'Rethinked Model']
accuracy_scores = [accuracy, accuracy_binary]

# Creating the plot
plt.bar(labels, accuracy_scores, color=['blue', 'green'])
plt.ylabel('Accuracy')
plt.title('Model Performance Comparison')
plt.show()

"""# CCCCCCCCCCCCCCCCCCCCCC"""

#import the required library
#from sklearn.model_selection import train_test_split

#split function. Define testing set is 20%
# Preparing the data: Separating features and target label
#X = dropout_df.drop('Student_Status', axis=1)
#y = dropout_df['Student_Status']

# Splitting the dataset into training and testing sets
#X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.2, random_state=42)

#import pandas as pd
#from sklearn.neural_network import MLPClassifier
#from sklearn.neighbors import KNeighborsClassifier
#from sklearn.svm import SVC
#from sklearn.gaussian_process import GaussianProcessClassifier
#from sklearn.ensemble import GradientBoostingClassifier
#from sklearn.gaussian_process.kernels import RBF
#from sklearn.tree import DecisionTreeClassifier
#from sklearn.ensemble import ExtraTreesClassifier
#from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
#from sklearn.naive_bayes import GaussianNB
#from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis
#from sklearn.linear_model import SGDClassifier

#names = ["Nearest_Neighbors", "Linear_SVM", "Polynomial_SVM", "RBF_SVM", "Gaussian_Process",
#"Gradient_Boosting", "Decision_Tree", "Extra_Trees", "Random_Forest", "Neural_Net", "AdaBoost",
#"Naive_Bayes", "QDA", "SGD"]

#classifiers = [KNeighborsClassifier(3),
#SVC(kernel="linear", C=0.025),
#SVC(kernel="poly", degree=3, C=0.025),
#SVC(kernel="rbf", C=1, gamma=2),
#GaussianProcessClassifier(1.0 * RBF(1.0)),
#GradientBoostingClassifier(n_estimators=100, learning_rate=1.0),
#DecisionTreeClassifier(max_depth=5),
#ExtraTreesClassifier(n_estimators=10, min_samples_split=2),
#RandomForestClassifier(max_depth=5, n_estimators=100),
#MLPClassifier(alpha=1, max_iter=1000),
#AdaBoostClassifier(n_estimators=100),
#GaussianNB(),
#QuadraticDiscriminantAnalysis(),
#SGDClassifier(loss="hinge", penalty="l2")]

#scores = []
#for name, clf in zip(names, classifiers):
#    clf.fit(X_train, Y_train)
#    score = clf.score(X_test, Y_test)
#    scores.append(score)

#scores

#dropout_df = pd.DataFrame()
#dropout_df['name'] = names
#dropout_df['score'] = scores
#dropout_df

#https://pandas.pydata.org/pandas-docs/stable/user_guide/style.html

#cm = sns.light_palette("green", as_cmap=True)
#s = dropout_df.style.background_gradient(cmap=cm)
#s

#sns.set(style="whitegrid")
#ax = sns.barplot(y="name", x="score", data=dropout_df)

# Create a DataFrame for the scores
#dropout_df = pd.DataFrame()
#dropout_df['name'] = names
#dropout_df['score'] = scores

# Set up the plot with a colormap that reflects the score intensity
#sns.set(style="whitegrid")
#plt.figure(figsize=(12, 8))

# Normalize the scores
#norm = plt.Normalize(dropout_df['score'].min(), dropout_df['score'].max())

# Create a color map (red to green)
#colors = plt.cm.RdYlGn(norm(dropout_df['score']))

# Convert the color map to a list for palette
#colors_list = colors.tolist()

# Create a bar plot with the color intensity based on the score
#ax = sns.barplot(y="name", x="score", data=dropout_df, palette=colors_list, dodge=False)

# Annotate bars with score values
#for index, value in enumerate(scores):
#    ax.text(value, index, f'{value:.2f}', color='black', ha="left", va="center")

# Add a color bar
#sm = plt.cm.ScalarMappable(cmap="RdYlGn", norm=norm)
#sm.set_array([])
#cbar = plt.colorbar(sm, ax=ax)
#cbar.ax.set_title('Score')

# Set labels and title
#plt.title('Classifier Accuracy Score')
#plt.xlabel('Accuracy Score')
#plt.ylabel('Classifier')
#plt.show()

# Create a DataFrame for the scores
#f1_scores = []
#for report in reports:
 #   lines = report.split('\n')
  #  for line in lines:
   #     if ('No Stroke' not in line) and ('Stroke' in line):
    #        parts = line.split()
     #       f1_score = float(parts[3])
      #      f1_scores.append(f1_score)
       #     break
#dropout_df = pd.DataFrame()
#dropout_df['name'] = names
#dropout_df['f1_score'] = f1_scores

# Set up the plot with a colormap that reflects the score intensity
#sns.set(style="whitegrid")
#plt.figure(figsize=(12, 8))

# Normalize the f1_scores_stroke
#norm = plt.Normalize(dropout_df['f1_score'].min(), dropout_df['f1_score'].max())

# Create a color map (red to green)
#colors = plt.cm.RdYlGn(norm(dropout_df['f1_score']))

# Convert the color map to a list for palette
#colors_list = colors.tolist()

# Create a bar plot with the color intensity based on the f1_score_stroke
#ax = sns.barplot(y="name", x="f1_score", data=dropout_df, palette=colors_list, dodge=False)

# Annotate bars with score values
#for index, value in enumerate(f1_scores):
  #  ax.text(value, index, f'{value:.2f}', color='black', ha="left", va="center")

# Add a color bar
#sm = plt.cm.ScalarMappable(cmap="RdYlGn", norm=norm)
#sm.set_array([])
#cbar = plt.colorbar(sm, ax=ax)
#cbar.ax.set_title('Score')

# Set labels and title
#plt.title('F1 Score of Classifier for Student Status Dropout')
#plt.xlabel('Dropout F1 Score')
#plt.ylabel('Classifier')
#plt.show()

#for i in range(len(names)):
 #   print(names[i])
  #  print("Overall accuracy:", round(scores[i], 4))
   # print(reports[i])